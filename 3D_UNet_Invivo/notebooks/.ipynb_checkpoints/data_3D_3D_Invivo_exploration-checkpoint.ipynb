{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and preprocessing\n",
    "\n",
    "- In this notebook, we load in the MRI scans and their segmentations, build a Dataset object for the train and test set.\n",
    "- Then we check some basic stats of the datasets and visualise a few scans.\n",
    "- Finally, we carry out our preprocessing steps and save the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-3e1b0da5688b>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3e1b0da5688b>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    sys.path.append('\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/amos/3D_3D_UNet_Invivo/')\n",
    "\n",
    "from src.data_utils import Dataset\n",
    "\n",
    "# get TF logger - set it to info for more tracking process\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test datasets exvivo\n",
    "\n",
    "Find all the raw data files and build train and test Dataset objects from the patients' scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# test_seg_files = glob('../data/raw/test/**/*.cso.dcm', recursive=True)\n",
    "# test_scan_files = [s.replace(\".cso\", \"_T1\") for s in test_seg_files]\n",
    "\n",
    "\n",
    "# #\n",
    "# train_seg_files = glob('../data/raw/train/**/*.cso.dcm', recursive=True)\n",
    "# train_scan_files = [s.replace(\".cso\", \"_T1\") for s in train_seg_files]\n",
    "\n",
    "\n",
    "# # build datasets from file paths\n",
    "# train_dataset = Dataset(scan_files=train_scan_files, seg_files=train_seg_files)\n",
    "# test_dataset = Dataset(scan_files=test_scan_files, seg_files=test_seg_files)\n",
    "\n",
    "\n",
    "# print('-------------------Test_data INFO-------------------------------')\n",
    "# print('----------------------------------------------------------------')\n",
    "# print('scan files in test_data: ' + str(len(test_dataset.scan_files)))\n",
    "# print('----------------------------------------------------------------')\n",
    "# print(test_dataset.scan_files)\n",
    "# print('----------------------------------------------------------------')\n",
    "# print('seg files in test_data: ' + str(len(test_dataset.seg_files)))\n",
    "# print('----------------------------------------------------------------')\n",
    "# print(test_dataset.seg_files)\n",
    "# print('----------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test datasets invivo\n",
    "\n",
    "Find all the raw data files and build train and test Dataset objects from the patients' scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training files\n",
    "_all_files = glob('../data/raw/train/**/*mask2_*.dcm', recursive=True)\n",
    "#_all_files = glob('../data/raw/train/**/*RIGHT.dcm', recursive=True)\n",
    "train_scan_files = [s.replace(\"_mask2_\", \"_t1_\") for s in _all_files]\n",
    "train_seg_files = [s.replace(\"_t1_\", \"_mask2_\") for s in train_scan_files]\n",
    "#train_scan_files = [s for s in train_scan_files if not 'plaque' in s]\n",
    "#train_seg_files = [s for s in train_seg_files if not 'plaque' in s]\n",
    "\n",
    "\n",
    "# evaluation files\n",
    "_all_files2 = glob('../data/raw/eval/**/*mask2_*.dcm', recursive=True)\n",
    "eval_scan_files = [s.replace(\"_mask2_\", \"_t1_\") for s in _all_files2]\n",
    "eval_seg_files = [s.replace(\"_t1_\", \"_mask2_\") for s in eval_scan_files]\n",
    "#eval_scan_files = [s for s in eval_scan_files if not 'plaque' in s]\n",
    "#eval_seg_files = [s for s in eval_seg_files if not 'plaque' in s]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testing files\n",
    "_all_files3 = glob('../data/raw/test/**/*mask2_*.dcm', recursive=True)\n",
    "test_scan_files = [s.replace(\"_mask2_\", \"_t1_\") for s in _all_files3]\n",
    "test_seg_files = [s.replace(\"_t1_\", \"_mask2_\") for s in test_scan_files]\n",
    "#test_scan_files = [s for s in test_scan_files if not 'plaque' in s]\n",
    "#test_seg_files = [s for s in test_seg_files if not 'plaque' in s]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# build datasets from file paths\n",
    "train_dataset = Dataset(scan_files=train_scan_files, seg_files=train_seg_files)\n",
    "eval_dataset = Dataset(scan_files=eval_scan_files, seg_files=eval_seg_files)\n",
    "test_dataset = Dataset(scan_files=test_scan_files, seg_files=test_seg_files)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_seg_files[0])\n",
    "print('-----------------')\n",
    "print(eval_scan_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_seg_files[0])\n",
    "print('-----------------')\n",
    "print(test_scan_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_seg_files[0])\n",
    "print('-----------------')\n",
    "print(train_scan_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_seg_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c38dc5d7a32f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seg_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scan_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_seg_files' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_seg_files))\n",
    "print('-----------------')\n",
    "print(len(train_scan_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eval_seg_files))\n",
    "print('-----------------')\n",
    "print(len(eval_scan_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_seg_files))\n",
    "print('-----------------')\n",
    "print(len(test_scan_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check basic stats\n",
    "Check number of patients in train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = len(train_dataset.patient_ids)\n",
    "eval_n = len(eval_dataset.patient_ids)\n",
    "test_n = len(test_dataset.patient_ids)\n",
    "\n",
    "train_scan_nums = [p.scans[0].shape[0] for p in train_dataset.patients.values()]\n",
    "eval_scan_nums = [p.scans[0].shape[0] for p in eval_dataset.patients.values()]\n",
    "test_scan_nums = [p.scans[0].shape[0] for p in test_dataset.patients.values()]\n",
    "\n",
    "print('Number of patients in train dataset: %d' % train_n)\n",
    "print('Number of patients in eval dataset: %d' % eval_n)\n",
    "print('Number of patients in test dataset: %d' % test_n)\n",
    "print('----------------------------------------------')\n",
    "print('Number of scans in train dataset: %d' % sum(train_scan_nums))\n",
    "print('Number of scans in eval dataset: %d' % sum(eval_scan_nums))\n",
    "print('Number of scans in test dataset: %d' % sum(test_scan_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of number of scans in train and test datasets.\n",
    "- They both seem bi-modal with roughly the same peaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].set_title('#scans_train_data ')\n",
    "ax[0].hist(train_scan_nums, bins=10)\n",
    "\n",
    "ax[1].set_title('#scans_eval_data ')\n",
    "ax[1].hist(eval_scan_nums, bins=10)\n",
    "\n",
    "ax[2].set_title('  #scans in test_dataset')\n",
    "ax[2].hist(test_scan_nums, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that none of the patients have scans from mixed manufacturers and with mixed slice thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract manufacturer and thickness sets from each patient\n",
    "# train_manufacturers = [p.manufacturers for p in train_dataset.patients.values()]\n",
    "# train_thicknesses = [p.thicknesses for p in train_dataset.patients.values()]\n",
    "\n",
    "# eval_manufacturers = [p.manufacturers for p in eval_dataset.patients.values()]\n",
    "# eval_thicknesses = [p.thicknesses for p in eval_dataset.patients.values()]\n",
    "\n",
    "# test_manufacturers = [p.manufacturers for p in test_dataset.patients.values()]\n",
    "# test_thicknesses = [p.thicknesses for p in test_dataset.patients.values()]\n",
    "\n",
    "# # check if any patient has slices from two different manufacturers or thicknesses - NO\n",
    "# for m in train_manufacturers + test_manufacturers:\n",
    "#     assert len(m) == 1\n",
    "    \n",
    "# for n in eval_manufacturers + eval_manufacturers:\n",
    "#     assert len(n) == 1\n",
    "\n",
    "# for t in train_thicknesses + test_thicknesses:\n",
    "#     assert len(t) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary table \n",
    "\n",
    "Collate all information into a pandas DataFrame from the datasets so we can analyse it easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collapse all list of sets to simple list\n",
    "# train_manufacturers = [list(i)[0] for i in train_manufacturers]\n",
    "# train_thicknesses = [list(i)[0] for i in train_thicknesses]\n",
    "\n",
    "# eval_manufacturers = [list(i)[0] for i in eval_manufacturers]\n",
    "# eval_thicknesses = [list(i)[0] for i in eval_thicknesses]\n",
    "\n",
    "# test_manufacturers = [list(i)[0] for i in test_manufacturers]\n",
    "# test_thicknesses = [list(i)[0] for i in test_thicknesses]\n",
    "\n",
    "# # extract scan width, height and max value\n",
    "# train_widths = [p.scans.shape[1] for p in train_dataset.patients.values()]\n",
    "# train_heights = [p.scans.shape[2] for p in train_dataset.patients.values()]\n",
    "# train_max = [p.scans.max() for p in train_dataset.patients.values()]\n",
    "\n",
    "# eval_widths = [p.scans.shape[1] for p in eval_dataset.patients.values()]\n",
    "# eval_heights = [p.scans.shape[2] for p in eval_dataset.patients.values()]\n",
    "# eval_max = [p.scans.max() for p in eval_dataset.patients.values()]\n",
    "\n",
    "\n",
    "# test_widths = [p.scans.shape[1] for p in test_dataset.patients.values()]\n",
    "# test_heights = [p.scans.shape[2] for p in test_dataset.patients.values()]\n",
    "# test_max = [p.scans.max() for p in test_dataset.patients.values()]\n",
    "\n",
    "# # calculate contingency table from them\n",
    "# df_summary = pd.DataFrame(\n",
    "#     list(\n",
    "#         zip(\n",
    "#             train_dataset.patient_ids +  eval_dataset.patient_ids + test_dataset.patient_ids,\n",
    "#             train_manufacturers + eval_manufacturers + test_manufacturers,\n",
    "#             train_thicknesses + eval_thicknesses + test_thicknesses,\n",
    "#             train_widths + eval_widths + test_widths,\n",
    "#             train_heights + eval_heights + test_heights,\n",
    "#             train_max + eval_max + test_max,\n",
    "#             train_scan_nums + eval_scan_nums + test_scan_nums,\n",
    "#             ['train'] * train_n + ['eval'] * eval_n + ['test'] * test_n\n",
    "#         )\n",
    "#     ), \n",
    "#     columns = ['patient_id', 'manufacturer', 'thickness', \n",
    "#                'width', 'heigth', 'max_val', 'scan_num', 'dataset']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the test and train datasets have been properly stratified with respect to manufacturer, i.e. half of the sample are from Siemens and half of them are from Philips.\n",
    "- However, the test dataset doesn't have slices of 4mm thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.drop(\n",
    "#     ['width', 'heigth', 'scan_num', 'max_val'], axis=1\n",
    "# ).groupby(\n",
    "#     ['dataset', 'manufacturer', 'thickness']\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Philips is the higher resolution machine, all scans are rectangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.drop(\n",
    "#     ['thickness', 'max_val', 'scan_num'], axis=1\n",
    "# ).groupby(\n",
    "#     ['dataset', 'manufacturer', 'width', 'heigth']\n",
    "# ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There's a large variation in the scans' maximum values and there are some clear outliers too (Siemens scan with max=65283, coming from the 18th scan of patient Prostate3T-01-0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.drop(\n",
    "#     ['thickness', 'patient_id', 'scan_num'], axis=1\n",
    "# ).groupby(\n",
    "#     ['dataset', 'manufacturer', 'width', 'heigth']\n",
    "# ).agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's the reason for those bi-modal histograms above, Philips mave higher number of scans on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.drop(\n",
    "#     ['thickness', 'max_val', 'width', 'heigth', 'patient_id'], axis=1\n",
    "# ).groupby(\n",
    "#     ['dataset', 'manufacturer']\n",
    "# ).agg(['min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate class frequency\n",
    "\n",
    "The 15 classes are imbalanced, calculate their frequency in the data. This will inform our weighting scheme that we use with the loss function at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = np.zeros(3)\n",
    "for i in range(len(train_dataset.patients.keys())):\n",
    "    patient_id = train_dataset.patient_ids[i]\n",
    "    \n",
    "    #TODO: cross check this result\n",
    "    segList = train_dataset.patients[patient_id].segs\n",
    "    for j, seg in enumerate(segList):\n",
    "        print(seg.shape)\n",
    "\n",
    "\n",
    "        class0  = np.count_nonzero(np.where(seg == 0))\n",
    "        class1  = np.count_nonzero(np.where(seg == 1))\n",
    "        class2  = np.count_nonzero(np.where(seg == 2))\n",
    "\n",
    "        print('class0: ' + str(class0))\n",
    "        print('.......................')\n",
    "        print('class1: ' + str(class1))\n",
    "        print('.......................')\n",
    "        print('class2: ' + str(class2))\n",
    "\n",
    "        class_freq += np.array([class0, class1, class2])\n",
    "    \n",
    "    \n",
    "class_freq = class_freq / class_freq.sum()\n",
    "#print(class_freq)  \n",
    "inv_class_freq = []\n",
    "for i in range(3):\n",
    "    if class_freq[i]==0:\n",
    "        inv_class_freq.append(0)\n",
    "    else:\n",
    "        inv_class_freq.append(1/class_freq[i])\n",
    "#print(inv_class_freq)\n",
    "norm_inv_class_freq = inv_class_freq / np.array(inv_class_freq).sum()\n",
    "norm_inv_class_freq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def revertOneHot(seg):\n",
    "#     seg2 = seg.copy()\n",
    "#     for classIndex in range(15):\n",
    "#         labelPositions = np.where(seg[:,:,:,classIndex]!=0)\n",
    "#         seg2[labelPositions] = classIndex\n",
    "#     return seg2\n",
    "\n",
    "\n",
    "# class_freq = np.zeros(15)\n",
    "# for i in range(len(train_dataset.patients.keys())):\n",
    "#     patient_id = train_dataset.patient_ids[i]\n",
    "    \n",
    "#     #TODO: cross check this result\n",
    "#     seg = train_dataset.patients[patient_id].segs\n",
    "#     #print(seg.shape)\n",
    "\n",
    " \n",
    "#     class0  = np.count_nonzero(np.where(seg == 0))\n",
    "#     class1  = np.count_nonzero(np.where(seg == 1))\n",
    "#     class2  = np.count_nonzero(np.where(seg == 2))\n",
    "#     class3  = np.count_nonzero(np.where(seg == 3))\n",
    "#     class4  = np.count_nonzero(np.where(seg == 4))\n",
    "#     class5  = np.count_nonzero(np.where(seg == 5))\n",
    "#     class6  = np.count_nonzero(np.where(seg == 6))\n",
    "#     class7  = np.count_nonzero(np.where(seg == 7))\n",
    "#     class8  = np.count_nonzero(np.where(seg == 8))\n",
    "#     class9  = np.count_nonzero(np.where(seg == 9))\n",
    "#     class10 = np.count_nonzero(np.where(seg == 10))\n",
    "#     class11 = np.count_nonzero(np.where(seg == 11))\n",
    "#     class12 = np.count_nonzero(np.where(seg == 12))\n",
    "#     class13 = np.count_nonzero(np.where(seg == 13))\n",
    "#     class14 = np.count_nonzero(np.where(seg == 14))\n",
    "    \n",
    "#     print('class1: ' + str(class1))\n",
    "#     print('.......................')\n",
    "#     print('class2: ' + str(class2))\n",
    "#     print('.......................')\n",
    "#     print('class3: ' + str(class3))\n",
    "#     print('.......................')\n",
    "#     print('class14: ' + str(class14))\n",
    "#     print('.......................')\n",
    "\n",
    "#     class_freq += np.array([class1, class2, class3,  class4,  class5,\n",
    "#                             class6,  class6,  class7,  class8,  class9,\n",
    "#                             class10,  class11,  class12,  class13,  class14])\n",
    "    \n",
    "    \n",
    "# class_freq = class_freq / class_freq.sum()\n",
    "# #print(class_freq)  \n",
    "# inv_class_freq = []\n",
    "# for i in range(15):\n",
    "#     if class_freq[i]==0:\n",
    "#         inv_class_freq.append(0)\n",
    "#     else:\n",
    "#         inv_class_freq.append(1/class_freq[i])\n",
    "# #print(inv_class_freq)\n",
    "# norm_inv_class_freq = inv_class_freq / np.array(inv_class_freq).sum()\n",
    "# norm_inv_class_freq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "As we've seen from the summary stats above the scans are non-normalised and span a wide range of maximal values, resolution and number of scans. \n",
    "\n",
    "The `preprocess_dataset` method:\n",
    "  - normalises the scans to be between zero and one\n",
    "  - resizes (and downsample) each scan and target segmentation image to the same width and height (128, 128)\n",
    "    - since 3D U-Net is fully convolutional, this isn't needed necessarily but it reduces the required memory at training time\n",
    "  - cap depth of scans, i.e. the number of scans across the patients: <32\n",
    "    - this ensures that with a 4 layer deep 3D U-Net we'll get matching dimensions when we concatenate the shurtcuts in the network \n",
    "    - we discard extra scans (i.e. more 32) and patients with less will be padded with zeros by TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, save preprocessed but non-resized eval and test dataset\n",
    "\n",
    "The performance evaluation has to be done on original (i.e. non rescaled images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code line\n",
    "eval_dataset_non_resized = copy.deepcopy(eval_dataset)\n",
    "test_dataset_non_resized = copy.deepcopy(test_dataset)\n",
    "train_dataset_non_resized = copy.deepcopy(train_dataset)\n",
    "\n",
    "\n",
    "patient_id = eval_dataset_non_resized.patient_ids[0]\n",
    "scans = eval_dataset_non_resized.patients[patient_id].scans \n",
    "segs = eval_dataset_non_resized.patients[patient_id].segs\n",
    "print(\"eval_data scans original shape: \", np.array(scans[0]).shape)\n",
    "print(\"eval_data seg original shape: \", np.array(segs[0]).shape)\n",
    "print(\".................................................................\")\n",
    "\n",
    "## added whille debugging\n",
    "patient_id = test_dataset_non_resized.patient_ids[0]\n",
    "scans = test_dataset_non_resized.patients[patient_id].scans \n",
    "segs = test_dataset_non_resized.patients[patient_id].segs\n",
    "print(\"test_data scans original shape: \", np.array(scans[0]).shape)\n",
    "print(\"test_data seg original shape: \", np.array(segs[0]).shape)\n",
    "print(\".................................................................\")\n",
    "\n",
    "patient_id = train_dataset_non_resized.patient_ids[0]\n",
    "scans = train_dataset_non_resized.patients[patient_id].scans \n",
    "segs = train_dataset_non_resized.patients[patient_id].segs\n",
    "print(\"train_data scans original shape: \", np.array(scans[0]).shape)\n",
    "print(\"train_data seg original shape: \", np.array(segs[0]).shape)\n",
    "print(\".................................................................\")\n",
    "\n",
    "\n",
    "# in case there is a need to resize inital data\n",
    "test_dataset_non_resized.preprocess_dataset(resize=True, width=256, height=256, max_scans=32)\n",
    "#test_dataset_non_resized.preprocess_dataset(width=32, height=32, max_scans=32)\n",
    "\n",
    "## added whille debugging\n",
    "# patient_id = test_dataset_non_resized.patient_ids[0]\n",
    "# scans = test_dataset_non_resized.patients[patient_id].scans\n",
    "# segs = test_dataset_non_resized.patients[patient_id].segs\n",
    "# print(\"test_data scans shape in non_resize_shapes: \", scans.shape)\n",
    "# print(\"test_data seg shape in non_resize_shapes: \", segs.shape)\n",
    "\n",
    "# save preprocessed but not resized eval and test dataset\n",
    "eval_dataset_non_resized.save_dataset('../data/processed/eval_dataset0.pckl')\n",
    "test_dataset_non_resized.save_dataset('../data/processed/test_dataset0.pckl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's resize and preprocess both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.preprocess_dataset(width=128, height=128, max_scans=32)\n",
    "eval_dataset.preprocess_dataset(width=128, height=128, max_scans=32)\n",
    "test_dataset.preprocess_dataset(width=128,height=128, max_scans=32)\n",
    "\n",
    "## eval\n",
    "patient_id = eval_dataset.patient_ids[0]\n",
    "scans = eval_dataset.patients[patient_id].scans\n",
    "segs = eval_dataset.patients[patient_id].segs\n",
    "print(\"eval_data scans shape after resizing: \", scans.shape)\n",
    "print(\"eval_data seg shape after resizing: \", segs.shape)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "## test\n",
    "patient_id = test_dataset.patient_ids[0]\n",
    "scans = test_dataset.patients[patient_id].scans\n",
    "segs = test_dataset.patients[patient_id].segs\n",
    "print(\"test_data scans shape after resizing: \", scans.shape)\n",
    "print(\"test_data seg shape after resizing: \", segs.shape)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "## train\n",
    "patient_id = train_dataset.patient_ids[0]\n",
    "scans = train_dataset.patients[patient_id].scans\n",
    "segs = train_dataset.patients[patient_id].segs\n",
    "print(\"train_data scans shape after resizing: \", scans.shape)\n",
    "print(\"train_data seg shape after resizing: \", segs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check voxel values of dicom scans and cso.dcm segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = train_dataset.patient_ids[0]\n",
    "scans = train_dataset.patients[patient_id].scans\n",
    "segs = train_dataset.patients[patient_id].segs\n",
    "print('Train_scans voxel values: ' + str(np.unique(scans)))\n",
    "print('Train_segs voxel values: ' + str(np.unique(segs)))\n",
    "print('---------------------------------------------------')\n",
    "\n",
    "patient_id = eval_dataset.patient_ids[0]\n",
    "scans = eval_dataset.patients[patient_id].scans\n",
    "segs = eval_dataset.patients[patient_id].segs\n",
    "print('Eval_scans voxel values: ' + str(np.unique(scans)))\n",
    "print('Eval_segs voxel values: ' + str(np.unique(segs)))\n",
    "print('---------------------------------------------------')\n",
    "\n",
    "\n",
    "patient_id = test_dataset.patient_ids[0]\n",
    "scans = test_dataset.patients[patient_id].scans\n",
    "segs = test_dataset.patients[patient_id].segs\n",
    "\n",
    "print('Test_scans voxel values: ' + str(np.unique(scans)))\n",
    "print('Test_segs voxel values: ' + str(np.unique(segs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise scans and respective segmentation files\n",
    "\n",
    "Each patient's scans can be viewed as an animation or as a tiled figure. Let's have a look at some of these.\n",
    "\n",
    "**Note** you'll need to re-execute the cell to watch the animation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lilli's approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = train_dataset.patient_ids[8]\n",
    "train_dataset.patients[patient_id].patient_plot_scans_segs()\n",
    "\n",
    "# for i in range(len(train_seg_files)):\n",
    "#     patient_id = train_dataset.patient_ids[i]\n",
    "#     train_dataset.patients[patient_id].patient_plot_scans_segs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = eval_dataset.patient_ids[0]\n",
    "eval_dataset.patients[patient_id].patient_plot_scans_segs()\n",
    "\n",
    "# for i in range(len(eval_seg_files)):\n",
    "#     patient_id = eval_dataset.patient_ids[i]\n",
    "#     eval_dataset.patients[patient_id].patient_plot_scans_segs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = test_dataset.patient_ids[0]\n",
    "test_dataset.patients[patient_id].patient_plot_scans_segs()\n",
    "\n",
    "# for i in range(len(test_seg_files)):\n",
    "#     patient_id = test_dataset.patient_ids[i]\n",
    "#     test_dataset.patients[patient_id].patient_plot_scans_segs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anim_scan approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = train_dataset.patient_ids[0]\n",
    "train_dataset.patients[patient_id].patient_anim_scans()\n",
    "\n",
    "# for i in range(len(train_seg_files)):\n",
    "#     patient_id = train_dataset.patient_ids[i]\n",
    "#     train_dataset.patients[patient_id].patient_plot_scans_segs()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = eval_dataset.patient_ids[0]\n",
    "eval_dataset.patients[patient_id].patient_anim_scans()\n",
    "\n",
    "# for i in range(len(eval_seg_files)):\n",
    "#     patient_id = eval_dataset.patient_ids[i]\n",
    "#     eval_dataset.patients[patient_id].patient_plot_scans_segs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = test_dataset.patient_ids[0]\n",
    "test_dataset.patients[patient_id].patient_anim_scans()\n",
    "\n",
    "# for i in range(len(test_seg_files)):\n",
    "#     patient_id = test_dataset.patient_ids[i]\n",
    "#     test_dataset.patients[patient_id].patient_plot_scans_segs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile_scan approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the scans and targets still look reasonable on the previous tiled example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note the target is now a one-hot tensor, we just show the class that we define in def tile_scans or def anim_scans\n",
    "patient_id = train_dataset.patient_ids[0]\n",
    "train_dataset.patients[patient_id].patient_tile_scans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the target is now a one-hot tensor, we just show the class that we define in def tile_scans or def anim_scans\n",
    "patient_id = eval_dataset.patient_ids[0]\n",
    "eval_dataset.patients[patient_id].patient_tile_scans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the target is now a one-hot tensor, we just show the class that we define in def tile_scans or def anim_scans\n",
    "patient_id = test_dataset.patient_ids[0]\n",
    "test_dataset.patients[patient_id].patient_tile_scans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check that the preprocessing worked for all images. Specifically we should find that:\n",
    "- the number of scans are 32 or less for all patients\n",
    "- the resolution is 128 by 128 \n",
    "- the maximum value of the scans is less than or equal to 1\n",
    "- the corresponding target tensor has an extra dimension, corresponding to the one hot encoding of the 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_dataset, eval_dataset, test_dataset]\n",
    "for dataset in datasets:\n",
    "    for i in range(len(dataset.patients.keys())):\n",
    "        patient_id = dataset.patient_ids[i]\n",
    "        scans = dataset.patients[patient_id].scans \n",
    "        segs = dataset.patients[patient_id].segs\n",
    "\n",
    "        assert(scans.shape[1:] == (128, 128))\n",
    "        assert(scans.shape[0] <= 32)\n",
    "        assert(scans.max() <= 1)\n",
    "        \n",
    "        assert(segs.shape[1:3] == (128, 128))\n",
    "        assert(segs.shape[0] <= 32)\n",
    "        assert(segs.shape[3] == 3)\n",
    "\n",
    "print(\"scans.shape: \", scans.shape)\n",
    "#print(\"scans.shape[1:]: \", scans.shape[1:])\n",
    "#print(\"scans.shape[0]: \", scans.shape[0])\n",
    "print(\"scans.max(): \", scans.max())\n",
    "\n",
    "print(\"..................................\")\n",
    "\n",
    "print(\"segs.shape: \", segs.shape)\n",
    "#print(\"segs.shape[1:3]: \", seg.shape[1:3])\n",
    "#print(\"segs.shape[1:]: \", seg.shape[0])\n",
    "#print(\"segs.shape[3]: \", seg.shape[3])\n",
    "print(\"segs.max(): \", segs.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resized datasets\n",
    "\n",
    "We save them as pickled objects so we can use them later for training and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_dataset('../data/processed/train_dataset_resized.pckl')\n",
    "eval_dataset.save_dataset('../data/processed/eval_dataset_resized.pckl')\n",
    "test_dataset.save_dataset('../data/processed/test_dataset_resized.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
