{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pydicom\n",
    "%pylab inline\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data,cmaps,label_names=None):\n",
    "    data_shape = np.array(data).shape\n",
    "    numImageTypes = data_shape[0]\n",
    "    numImageSets = data_shape[1]\n",
    "    print(numImageTypes,numImageSets)\n",
    "    fig = plt.figure(figsize=(3*numImageTypes,3*numImageSets))\n",
    "    for i in range(numImageSets):\n",
    "        for j in range(numImageTypes):\n",
    "            ax = plt.subplot(numImageSets, numImageTypes,i*numImageTypes + j+1)\n",
    "            if not label_names==None:\n",
    "                ax.set_title(label_names[j])\n",
    "            plt.axis('off')\n",
    "            img = plt.imshow(data[j][i])\n",
    "            img.set_cmap(cmaps[j])\n",
    "    plt.show()\n",
    "\n",
    "def plot_prepared_data(prepared_data,labels,label_names=None):\n",
    "    cmaps = ['gray','gist_ncar']\n",
    "    data = [prepared_data[0][:,:,:,0], one_hot_decoding(prepared_data[1],labels)]\n",
    "    data_shape = np.array(data).shape\n",
    "    numImageTypes = data_shape[0]\n",
    "    numImageSets = data_shape[1]\n",
    "    print(numImageTypes,numImageSets)\n",
    "    fig = plt.figure(figsize=(3*numImageTypes,3*numImageSets))\n",
    "    for i in range(numImageSets):\n",
    "        for j in range(numImageTypes):\n",
    "            ax = plt.subplot(numImageSets, numImageTypes,i*numImageTypes + j+1)\n",
    "            if not label_names==None:\n",
    "                ax.set_title(label_names[j])\n",
    "            plt.axis('off')\n",
    "            img = plt.imshow(data[j][i])\n",
    "            img.set_cmap(cmaps[j])\n",
    "    plt.savefig('prepared_data.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import utils\n",
    "def fix_labels2(label_images,labels):\n",
    "    encoded = []\n",
    "    numLabels = len(labels)\n",
    "    for img in label_images:\n",
    "        img2 = numpy.zeros_like(img)\n",
    "        for i,label in enumerate(labels):\n",
    "            img2 = np.where(img==label, i+1, img2)\n",
    "        encoded.append(img2)\n",
    "    return encoded\n",
    "def fix_labels(img,labels):\n",
    "    img2 = numpy.zeros_like(img)\n",
    "    for i,label in enumerate(labels):\n",
    "        img2 = np.where(img==label, i+1, img2)        \n",
    "    return img2\n",
    "        \n",
    "def one_hot_encoding(label_images,numLabels):\n",
    "    encoded = []\n",
    "    for img in label_images:\n",
    "        original_shape = img.shape\n",
    "        #print(original_shape)\n",
    "        #print(numLabels)\n",
    "        img2= utils.to_categorical(img.flatten(), numLabels)\n",
    "        img2 = np.reshape(img2, (original_shape[0],original_shape[1],numLabels))\n",
    "        encoded.append(img2)\n",
    "        #print(numLabels,img2.shape)\n",
    "    return encoded\n",
    "\n",
    "        \n",
    "def one_hot_decoding(encodings,labels):\n",
    "    decoded = []\n",
    "    for img in encodings:\n",
    "        if (len(img.shape)!=3):\n",
    "            print(\"ERROR: wrong shape\")\n",
    "        original_shape = (img.shape[0],img.shape[1])\n",
    "        img2 = numpy.zeros(original_shape)\n",
    "        \n",
    "        #print(original_shape)\n",
    "        for i, label in enumerate(labels):\n",
    "            img2 += np.array(img)[:,:,i+1]*label\n",
    "        decoded.append(img2)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw'\n",
    "path_train = path + \"/train\"\n",
    "path_eval  = path + \"/eval\"\n",
    "path_test  = path + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    #print(path)\n",
    "    train_gt = glob.glob(path + \"/*.cso.dcm\")\n",
    "    train_T1 = [ s.replace(\".cso\", \"_T1\") for s in train_gt]\n",
    "    train_T2 = [ s.replace(\".cso\", \"_T2\") for s in train_gt]\n",
    "    train_T2s = [ s.replace(\".cso\", \"_T2s\") for s in train_gt]\n",
    "    train_PD = [ s.replace(\".cso\", \"_PD\") for s in train_gt]\n",
    "    #print(train_gt)\n",
    "    gts = []\n",
    "    t1s = []\n",
    "    t2s = []\n",
    "    t2ss = []\n",
    "    pds = []\n",
    "    for s in train_gt:\n",
    "        ds = pydicom.dcmread(s) \n",
    "        gts.append(ds.pixel_array)\n",
    "    for s in train_T1:\n",
    "        ds = pydicom.dcmread(s) \n",
    "        t1s.append(ds.pixel_array)\n",
    "    for s in train_T2:\n",
    "        ds = pydicom.dcmread(s) \n",
    "        t2s.append(ds.pixel_array)\n",
    "    for s in train_T2s:\n",
    "        ds = pydicom.dcmread(s) \n",
    "        t2ss.append(ds.pixel_array)\n",
    "    for s in train_PD:\n",
    "        ds = pydicom.dcmread(s) \n",
    "        pds.append(ds.pixel_array)\n",
    "    \n",
    "    return t1s,t2s,t2ss,pds,gts\n",
    "\n",
    "def combine_contrasts(contrasts):\n",
    "    contrasts = [np.array(c) for c in contrasts]\n",
    "    #print(\"c\",contrasts[1].shape)\n",
    "    combined_contrasts = np.stack(contrasts,axis=3)\n",
    "    #print(\"d\",combined_contrasts.shape)\n",
    "    return combined_contrasts\n",
    "\n",
    "def prepare_data(path,labels):\n",
    "    print('Labels: '+ str(labels))\n",
    "    t1s,t2s,t2ss,pds,gts = read_data(path)\n",
    "    #print(\"gts:\",np.unique(gts[0]),np.unique(gts[1]))\n",
    "    data_x = combine_contrasts([t1s,t2s,t2ss,pds])\n",
    "    #print(\"e\",train_x.shape,len(labels)+1)\n",
    "    data_y = np.array(one_hot_encoding(fix_labels2(gts,labels),len(labels)+1))#labels,15))#\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLabels = {\"lumen\": 14, \n",
    "             \"intima\":  1 , \n",
    "             \"media\":  2 , \n",
    "             \"AtheromatousCore\":  4 , \n",
    "             \"FibrousTissue\":  3 , \n",
    "             \"calcification\": 13 ,\n",
    "             \"WhiteThrombus\": 6 ,\n",
    "             \"RedThrombus\":  7 ,\n",
    "             \"microvessels\":  11 ,\n",
    "             \"hemorrhage\":  12 ,\n",
    "             \"unknown\":  8 ,\n",
    "             \"inflammation\":  10 ,\n",
    "             \"fibrin+hemorrhage\":  9 ,\n",
    "             \"atheromatous+hemorrhage\":  5 ,\n",
    "             \"background\": 0}\n",
    "id_to_label = {v: k for k, v in allLabels.items()}\n",
    "\n",
    "labels_to_keep = [1,2,5,8,13,14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 2, 5, 8, 13, 14]\n",
      "Training data:\n",
      "x shape: (512, 512, 4)\n",
      "y shape: (512, 512, 7)\n",
      "-------------------------\n",
      "Labels: [1, 2, 5, 8, 13, 14]\n",
      "Evaluation data:\n",
      "x shape: (512, 512, 4)\n",
      "y shape: (512, 512, 7)\n",
      "-------------------------\n",
      "Test data: \n",
      "Labels: [1, 2, 5, 8, 13, 14]\n",
      "x shape: (512, 512, 4)\n",
      "y shape: (512, 512, 7)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = prepare_data(path_train,labels_to_keep)\n",
    "print('Training data:')\n",
    "print(\"x shape:\",train_x[0].shape)\n",
    "print(\"y shape:\",train_y[0].shape)\n",
    "print('-------------------------')\n",
    "\n",
    "eval_x, eval_y = prepare_data(path_eval,labels_to_keep)\n",
    "print('Evaluation data:')\n",
    "print(\"x shape:\",eval_x[0].shape)\n",
    "print(\"y shape:\",eval_y[0].shape)\n",
    "print('-------------------------')\n",
    "\n",
    "print('Test data: ')\n",
    "test_x, test_y = prepare_data(path_test,labels_to_keep)\n",
    "print(\"x shape:\",train_x[0].shape)\n",
    "print(\"y shape:\",train_y[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 51\n"
     ]
    }
   ],
   "source": [
    "plot_prepared_data([train_x,train_y],labels_to_keep)\n",
    "\n",
    "#plot_prepared_data([eval_x,eval_y],labels_to_keep)\n",
    "#plot_prepared_data([test_x,test_y],labels_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1s,t2s,t2ss,pds,gts = read_data(path_train)\n",
    "plot_data([t1s,t2s,t2ss,pds,gts],['gray','gray','gray','gray','gist_ncar']) #fix_labels2(labels,[1,14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as k\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, GlobalMaxPool2D,concatenate, add\n",
    "#from tensorflow.keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "#from tensorflow.keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "#from tensorflow.keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "#from tensorflow.keras.layers.merge import concatenate, add\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return Conv2DTranspose(filters, kernel_size, strides=2, padding=padding)#strides=strides,\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return UpSampling2D(kernel_size)\n",
    "\n",
    "\n",
    "def conv2d_block(\n",
    "    inputs, \n",
    "    use_batch_norm=True, \n",
    "    dropout=0.3, \n",
    "    filters=16, \n",
    "    kernel_size=(3,3), \n",
    "    activation='relu', \n",
    "    kernel_initializer='he_normal', \n",
    "    padding='same'):\n",
    "    \n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (inputs)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    #if dropout > 0.0:\n",
    "    c = Dropout(dropout)(c)\n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer, padding=padding) (c)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    return c\n",
    "\n",
    "def normal_unet(\n",
    "    inputs,\n",
    "    num_classes=1,\n",
    "    num_channels=1,\n",
    "    use_batch_norm=True, \n",
    "    upsample_mode='deconv', #'simple', #  'deconv' or \n",
    "    use_dropout_on_upsampling=False, \n",
    "    dropout=0.5, \n",
    "    dropout_change_per_layer=0.0,\n",
    "    filters=16,\n",
    "    num_layers=4,\n",
    "    output_activation='softmax'): # 'sigmoid' or 'softmax'\n",
    "    print(\"--------create unet\")\n",
    "    if upsample_mode=='deconv':\n",
    "        upsample=upsample_conv\n",
    "    else:\n",
    "        upsample=upsample_simple\n",
    "    keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "    # Build U-Net model\n",
    "    #inputs = Input( shape=(num_channels,) +tuple(input_shape), name=\"input\") #\n",
    "    x = inputs   \n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=use_batch_norm, dropout=dropout)\n",
    "        print(\" -v- \",filters)\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2)) (x)\n",
    "        dropout += dropout_change_per_layer\n",
    "        filters = filters*2 # double the number of filters with each layer\n",
    "\n",
    "    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=use_batch_norm, dropout=dropout)\n",
    "    print(\" -+- \",filters)\n",
    "    print(down_layers)\n",
    "\n",
    "    if not use_dropout_on_upsampling:\n",
    "        dropout = 0.0\n",
    "        dropout_change_per_layer = 0.0\n",
    "\n",
    "    for conv in reversed(down_layers):        \n",
    "        filters //= 2 # decreasing number of filters with each layer \n",
    "        dropout -= dropout_change_per_layer\n",
    "        x = upsample(filters, (2,2), strides=(2, 2), padding='same') (x)#(2, 2)\n",
    "        print(\" -^- \",filters)\n",
    "        x = tf.keras.layers.concatenate([x, conv],axis=3)\n",
    "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=use_batch_norm, dropout=dropout)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1),  activation='softmax', name=\"output\") (x)    \n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    print(\"num_classes\",num_classes)\n",
    "    \n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = unet()\n",
    "#model_checkpoint = ModelCheckpoint('unet_test.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#tf.keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "print(train_x.shape,train_y.shape)\n",
    "input_img = tf.keras.Input(train_x[0].shape, name='img')\n",
    "#model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True,num_classes=5)\n",
    "model = normal_unet(input_img, filters=32, dropout=0.5,num_classes=len(labels_to_keep)+1,num_channels=4,num_layers=4)\n",
    "#model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "#model.fit_generator([t1s,data_y],steps_per_epoch=2000,epochs=5,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pred(y_true, y_pred):\n",
    "    return k.mean(y_pred)\n",
    "\n",
    "def compute_ious(gt, predictions):\n",
    "    gt_ = get_segmentations(gt)\n",
    "    predictions_ = get_segmentations(predictions)\n",
    "\n",
    "    if len(gt_) == 0 and len(predictions_) == 0:\n",
    "        return np.ones((1, 1))\n",
    "    elif len(gt_) != 0 and len(predictions_) == 0:\n",
    "        return np.zeros((1, 1))\n",
    "    else:\n",
    "        iscrowd = [0 for _ in predictions_]\n",
    "        ious = cocomask.iou(gt_, predictions_, iscrowd)\n",
    "        if not np.array(ious).size:\n",
    "            ious = np.zeros((1, 1))\n",
    "        return ious\n",
    "\n",
    "def intersection_over_union(y_true, y_pred):\n",
    "    ious = []\n",
    "    for y_t, y_p in enumerate (zip(y_true, y_pred)):\n",
    "        iou = compute_ious(y_t, y_p)\n",
    "        iou_mean = 1.0 * np.sum(iou) / len(iou)\n",
    "        ious.append(iou_mean)\n",
    "    return np.mean(ious)\n",
    "\n",
    "\n",
    "def intersection_over_union_thresholds(y_true, y_pred):\n",
    "    iouts = []\n",
    "    for y_t, y_p in list(zip(y_true, y_pred)):\n",
    "        iouts.append(compute_eval_metric(y_t, y_p))\n",
    "    return np.mean(iouts)\n",
    "\n",
    "def iou_loss_core(y_true, y_pred, smooth=1):\n",
    "    intersection = k.sum(k.abs(y_true * y_pred), axis=-1)\n",
    "    union = k.sum(y_true,-1) + k.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "# not working\n",
    "def sparse_Mean_IOU(y_true, y_pred):\n",
    "    nb_classes = k.int_shape(y_pred)[-1]\n",
    "    iou = []\n",
    "    pred_pixels = k.argmax(y_pred, axis=-1)\n",
    "    for i in range(0, nb_classes): # exclude first label (background) and last label (void)\n",
    "        true_labels = k.equal(y_true[:,:,0], i)\n",
    "        pred_labels = k.equal(pred_pixels, i)\n",
    "        inter = tf.to_int32(true_labels & pred_labels)\n",
    "        union = tf.to_int32(true_labels | pred_labels)\n",
    "        legal_batches = k.sum(tf.to_int32(true_labels), axis=1)>0\n",
    "        ious = k.sum(inter, axis=1)/k.sum(union, axis=1)\n",
    "        iou.append(k.mean(tf.gather(ious, indices=tf.where(legal_batches)))) # returns average IoU of the same objects\n",
    "    iou = tf.stack(iou)\n",
    "    legal_labels = ~tf.debugging.is_nan(iou)\n",
    "    iou = tf.gather(iou, indices=tf.where(legal_labels))\n",
    "    return k.mean(iou)\n",
    "\n",
    "\n",
    "def Mean_IOU_tensorflow_1(y_true, y_pred):\n",
    "    nb_classes = k.int_shape(y_pred)[-1]\n",
    "    iou = []\n",
    "    true_pixels = k.argmax(y_true, axis=-1)\n",
    "    pred_pixels = k.argmax(y_pred, axis=-1)\n",
    "    void_labels = k.equal(k.sum(y_true, axis=-1), 0)\n",
    "    for i in range(0, nb_classes): # exclude first label (background) and last label (void)\n",
    "        true_labels = k.equal(true_pixels, i) & ~void_labels\n",
    "        pred_labels = k.equal(pred_pixels, i) & ~void_labels\n",
    "        inter = tf.to_int32(true_labels & pred_labels)\n",
    "        union = tf.to_int32(true_labels | pred_labels)\n",
    "        legal_batches = k.sum(tf.to_int32(true_labels), axis=1)>0\n",
    "        ious = k.sum(inter, axis=1)/k.sum(union, axis=1)\n",
    "        iou.append(k.mean(tf.gather(ious, indices=tf.where(legal_batches)))) # returns average IoU of the same objects\n",
    "    iou = tf.stack(iou)\n",
    "    legal_labels = ~tf.debugging.is_nan(iou)\n",
    "    iou = tf.gather(iou, indices=tf.where(legal_labels))\n",
    "    return k.mean(iou)\n",
    "\n",
    "\n",
    "def Mean_IOU_tensorflow_2(y_true, y_pred):\n",
    "    nb_classes = k.int_shape(y_pred)[-1]\n",
    "    iou = []\n",
    "    true_pixels = k.argmax(y_true, axis=-1)\n",
    "    pred_pixels = k.argmax(y_pred, axis=-1)\n",
    "    void_labels = k.equal(k.sum(y_true, axis=-1), 0)\n",
    "    for i in range(0, nb_classes): # exclude first label (background) and last label (void)\n",
    "        true_labels = k.equal(true_pixels, i) & ~void_labels\n",
    "        pred_labels = k.equal(pred_pixels, i) & ~void_labels\n",
    "        inter = tf.to_int32(true_labels & pred_labels)\n",
    "        union = tf.to_int32(true_labels | pred_labels)\n",
    "        legal_batches = k.sum(tf.to_int32(true_labels), axis=1)>0\n",
    "        ious = k.sum(inter, axis=1)/k.sum(union, axis=1)\n",
    "        iou.append(K.mean(ious[legal_batches]))\n",
    "    iou = tf.stack(iou)\n",
    "    legal_labels = ~tf.math.is_nan(iou)\n",
    "    iou = iou[legal_labels]\n",
    "    return k.mean(iou)\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    \"\"\"Jaccard distance for semantic segmentation.\n",
    "    Also known as the intersection-over-union loss.\n",
    "    This loss is useful when you have unbalanced numbers of pixels within an image\n",
    "    because it gives all classes equal weight. However, it is not the defacto\n",
    "    standard for image segmentation.\n",
    "    For example, assume you are trying to predict if\n",
    "    each pixel is cat, dog, or background.\n",
    "    You have 80% background pixels, 10% dog, and 10% cat.\n",
    "    If the model predicts 100% background\n",
    "    should it be be 80% right (as with categorical cross entropy)\n",
    "    or 30% (with this loss)?\n",
    "    The loss has been modified to have a smooth gradient as it converges on zero.\n",
    "    This has been shifted so it converges on 0 and is smoothed to avoid exploding\n",
    "    or disappearing gradient.\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    # Arguments\n",
    "        y_true: The ground truth tensor.\n",
    "        y_pred: The predicted tensor\n",
    "        smooth: Smoothing factor. Default is 100.\n",
    "    # Returns\n",
    "        The Jaccard distance between the two tensors.\n",
    "    # References\n",
    "        - [What is a good evaluation measure for semantic segmentation?](\n",
    "           http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf)\n",
    "    \"\"\"\n",
    "    intersection = k.sum(k.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = k.sum(k.abs(y_true) + k.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dice_ratio(pred, label):\n",
    "    '''Note: pred & label should only contain 0 or 1.\n",
    "    '''   \n",
    "    return np.sum(pred[label==1])*2.0 / (np.sum(pred) + np.sum(label))\n",
    "\n",
    "\n",
    "def iou_new(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = k.flatten(y_true)\n",
    "    y_pred_f = k.flatten(y_pred)\n",
    "    intersection = k.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (k.sum(y_true_f) + k.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "    \n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = k.sum(y_true * y_pred)\n",
    "    union = k.sum(y_true + y_pred)\n",
    "    jac = (intersection + 1.) / (union - intersection + 1.)\n",
    "    return k.mean(jac)\n",
    "\n",
    "\n",
    "def threshold_binarize(x, threshold=0.5):\n",
    "    ge = tf.greater_equal(x, tf.constant(threshold))\n",
    "    y = tf.where(ge, x=tf.ones_like(x), y=tf.zeros_like(x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def iou_thresholded(y_true, y_pred, threshold=0.5, smooth=1.):\n",
    "    y_pred = threshold_binarize(y_pred, threshold)\n",
    "    y_true_f = k.flatten(y_true)\n",
    "    y_pred_f = k.flatten(y_pred)\n",
    "    intersection = k.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (k.sum(y_true_f) + k.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = k.flatten(y_true)\n",
    "    y_pred_f = k.flatten(y_pred)\n",
    "    intersection = k.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (\n",
    "                k.sum(y_true_f) + k.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return -dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/danielenricocahall/Keras-Weighted-Hausdorff-Distance-Loss/blob/master/hausdorff/hausdorff.py\n",
    "# https://github.com/N0vel/weighted-hausdorff-distance-tensorflow-keras-loss\n",
    "# https://jeune-research.tistory.com/entry/Loss-Functions-for-Image-Segmentation-Distance-Based-Losses\n",
    "\n",
    "# matlibplot boxplot\n",
    "# https://www.programcreek.com/python/example/102337/matplotlib.pyplot.boxplot\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "\n",
    "def iou(y_true, y_pred):  #this can be used as a loss if you make it negative\n",
    "    intersection = y_true * y_pred\n",
    "    notTrue = 1 - y_true\n",
    "    union = y_true + (notTrue * y_pred)\n",
    "\n",
    "    return (k.sum(intersection, axis=-1) + k.epsilon()) / (k.sum(union, axis=-1) + k.epsilon())\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = k.sum(k.round(k.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = k.sum(k.round(k.clip(y_pred, 0, 1)))\n",
    "    c3 = k.sum(k.round(k.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = k.ones_like(y_true) \n",
    "    true_positives = k.sum(k.round(k.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = k.sum(k.round(k.clip(y_true, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (all_positives + k.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = k.ones_like(y_true) \n",
    "    true_positives = k.sum(k.round(k.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = k.sum(k.round(k.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + k.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+k.epsilon()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", iou, f1_score, precision, recall])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=['accuracy', Mean_IOU_tensorflow_1, jaccard_coef, recall, precision, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [\n",
    "#    EarlyStopping(patience=100, verbose=1),\n",
    "#    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "#    ModelCheckpoint('model-test.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "#]\n",
    "#class_weight = {0 : 1., 1: 20.}\n",
    "results = model.fit(train_x,train_y, batch_size=4, epochs=500, \n",
    "                    validation_data=(eval_x, eval_y))#callbacks=callbacks,,class_weight=class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print history content\n",
    "# print(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(results.history['acc'])\n",
    "plt.plot(results.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig('Model_Accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig('Model_Loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation mean_iou values\n",
    "plt.plot(results.history['Mean_IOU_tensorflow_1'])\n",
    "plt.plot(results.history['val_Mean_IOU_tensorflow_1'])\n",
    "plt.title('Model IOU')\n",
    "plt.ylabel('IOU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig('Model_IOU.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation mean_iou values\n",
    "plt.plot(results.history['jaccard_coef'])\n",
    "plt.plot(results.history['val_jaccard_coef'])\n",
    "plt.title('Model Jaccard Coef')\n",
    "plt.ylabel('Jaccard_coef')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig('Jaccard_coef.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation mean_iou values\n",
    "plt.plot(results.history['f1_score'])\n",
    "plt.plot(results.history['val_f1_score'])\n",
    "plt.title('Model F1_score')\n",
    "plt.ylabel('F1_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.savefig('F1_score.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_prediction1(data,cmaps,label_names=None):\n",
    "    data_shape = np.array(data).shape\n",
    "    numImageTypes = data_shape[0]\n",
    "    numImageSets = data_shape[1]\n",
    "    print(numImageTypes,numImageSets)\n",
    "    fig = plt.figure(figsize=(3*numImageTypes,3*numImageSets))\n",
    "    for i in range(numImageSets):\n",
    "        for j in range(numImageTypes):\n",
    "            ax = plt.subplot(numImageSets, numImageTypes,i*numImageTypes + j+1)\n",
    "            if not label_names==None:\n",
    "                ax.set_title(label_names[j])\n",
    "            plt.axis('off')\n",
    "            img = plt.imshow(data[j][i])\n",
    "            img.set_cmap(cmaps[j])\n",
    "    plt.savefig('prediction_display1')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_data_prediction2(data,cmaps,label_names=None):\n",
    "    data_shape = np.array(data).shape\n",
    "    numImageTypes = data_shape[0]\n",
    "    numImageSets = data_shape[1]\n",
    "    print(numImageTypes,numImageSets)\n",
    "    fig = plt.figure(figsize=(3*numImageTypes,3*numImageSets))\n",
    "    for i in range(numImageSets):\n",
    "        for j in range(numImageTypes):\n",
    "            ax = plt.subplot(numImageSets, numImageTypes,i*numImageTypes + j+1)\n",
    "            if not label_names==None:\n",
    "                ax.set_title(label_names[j])\n",
    "            plt.axis('off')\n",
    "            img = plt.imshow(data[j][i])\n",
    "            img.set_cmap(cmaps[j])\n",
    "    plt.savefig('prediction_display2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ys = model.predict(np.array(test_x).astype(float))\n",
    "\n",
    "def one_hot_decoding_max(encodings,labels): #choose class with highest probability\n",
    "    decoded = []\n",
    "    labels_with_bg = [0] + labels\n",
    "    for img in encodings:\n",
    "        if (len(img.shape)!=3):\n",
    "            print(\"ERROR: wrong shape\")\n",
    "        original_shape = (img.shape[0],img.shape[1])\n",
    "        img2 = np.argmax(img,axis=2)\n",
    "        img3 = numpy.zeros_like(img2)\n",
    "        for i,label in enumerate(labels_with_bg):\n",
    "            img3 = np.where(img2==i, label, img3)\n",
    "        decoded.append(img3)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "test_ys_undec = one_hot_decoding_max(test_ys,labels_to_keep) #prediction\n",
    "test_y_undec = one_hot_decoding(test_y,labels_to_keep) #gt\n",
    "\n",
    "plot_data_prediction1([test_x[:,:,:,0],test_x[:,:,:,1],test_x[:,:,:,2],test_x[:,:,:,3],test_y_undec,test_ys_undec],['gray','gray','gray','gray','gist_ncar','gist_ncar'],label_names=[\"t1\",\"t2\",\"t2s\",\"pd\",\"gt\",\"prediction\"])\n",
    "\n",
    "labels_with_bg = [0] + labels_to_keep\n",
    "test_ys_list = [test_ys[:,:,:,i] for i in range(len(labels_with_bg))]\n",
    "test_ys_labels = [id_to_label[l] for l in labels_with_bg]\n",
    "colour_maps = ['hot' for i in range(len(labels_with_bg))]\n",
    "plot_data_prediction2(test_ys_list,colour_maps,label_names=test_ys_labels)\n",
    "\n",
    "#plot_data([test_ys[:,:,:,0],test_ys[:,:,:,1],test_ys[:,:,:,2],test_ys[:,:,:,3],test_ys[:,:,:,4],test_ys[:,:,:,5]],['hot','hot','hot','hot','hot','hot'],label_names=[\"bg\",\"intima\",\"atheromatous\",\"hem\",\"calc\",\"lumen\"])\n",
    "#print(test_ys_undec[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
